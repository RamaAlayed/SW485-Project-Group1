{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dfb44a3-1bd0-4049-b86e-2fa0f17a60cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in d:\\anaconda\\lib\\site-packages (0.21.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\anaconda\\lib\\site-packages (from tokenizers) (0.29.2)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tokenizers --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "396ff626-f899-455d-bfaa-f76e115dd291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Sample:\n",
      "    Rank                    Title                     Genre  \\\n",
      "0     1  Guardians of the Galaxy   Action,Adventure,Sci-Fi   \n",
      "1     2               Prometheus  Adventure,Mystery,Sci-Fi   \n",
      "2     3                    Split           Horror,Thriller   \n",
      "3     4                     Sing   Animation,Comedy,Family   \n",
      "4     5            Suicide Squad  Action,Adventure,Fantasy   \n",
      "\n",
      "                                         Description              Director  \\\n",
      "0  A group of intergalactic criminals are forced ...            James Gunn   \n",
      "1  Following clues to the origin of mankind, a te...          Ridley Scott   \n",
      "2  Three girls are kidnapped by a man with a diag...    M. Night Shyamalan   \n",
      "3  In a city of humanoid animals, a hustling thea...  Christophe Lourdelet   \n",
      "4  A secret government agency recruits some of th...            David Ayer   \n",
      "\n",
      "                                              Actors  Year  Runtime (Minutes)  \\\n",
      "0  Chris Pratt, Vin Diesel, Bradley Cooper, Zoe S...  2014                121   \n",
      "1  Noomi Rapace, Logan Marshall-Green, Michael Fa...  2012                124   \n",
      "2  James McAvoy, Anya Taylor-Joy, Haley Lu Richar...  2016                117   \n",
      "3  Matthew McConaughey,Reese Witherspoon, Seth Ma...  2016                108   \n",
      "4  Will Smith, Jared Leto, Margot Robbie, Viola D...  2016                123   \n",
      "\n",
      "   Rating   Votes  Revenue (Millions)  Metascore  \n",
      "0     8.1  757074              333.13       76.0  \n",
      "1     7.0  485820              126.46       65.0  \n",
      "2     7.3  157606              138.12       62.0  \n",
      "3     7.2   60545              270.32       59.0  \n",
      "4     6.2  393727              325.02       40.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum PyNormalizerTypeWrapper at line 67 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Step 3: Load OpenChat 3.5 Model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenchat/openchat-3.5-0106\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 22\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     25\u001b[0m generator \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:727\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    724\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    725\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    726\u001b[0m         )\n\u001b[1;32m--> 727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    729\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1854\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1852\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   1855\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   1856\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   1857\u001b[0m     init_configuration,\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   1859\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1860\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1861\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1862\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   1863\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   1864\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1865\u001b[0m )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2017\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2015\u001b[0m \u001b[38;5;66;03m# Instantiate tokenizer.\u001b[39;00m\n\u001b[0;32m   2016\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2017\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2018\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2019\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2021\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2022\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\transformers\\models\\llama\\tokenization_llama_fast.py:116\u001b[0m, in \u001b[0;36mLlamaTokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    105\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    115\u001b[0m ):\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    117\u001b[0m         vocab_file\u001b[38;5;241m=\u001b[39mvocab_file,\n\u001b[0;32m    118\u001b[0m         tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file,\n\u001b[0;32m    119\u001b[0m         clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m    120\u001b[0m         unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[0;32m    121\u001b[0m         bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[0;32m    122\u001b[0m         eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[0;32m    123\u001b[0m         use_default_system_prompt\u001b[38;5;241m=\u001b[39muse_default_system_prompt,\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    125\u001b[0m     )\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:111\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_file(fast_tokenizer_file)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[1;31mException\u001b[0m: data did not match any variant of untagged enum PyNormalizerTypeWrapper at line 67 column 3"
     ]
    }
   ],
   "source": [
    "# Generative AI Phase 4 Notebook\n",
    "# ============================================\n",
    "# Movie Dataset Analysis with OpenChat 3.5 (via HuggingFace Transformers)\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load your movie dataset \n",
    "data = pd.read_csv(\"D:/Assignments/imdb_movie_dataset (2).csv\")\n",
    "print(\"Dataset Sample:\\n\", data.head())\n",
    "\n",
    "# Step 2: Choose two templates for generating advice\n",
    "TEMPLATE_1 = \"As a professional movie critic, analyze this movie dataset and suggest patterns in the genre, rating, and year: {user_input}\"\n",
    "TEMPLATE_2 = \"Imagine you are an AI data scientist. Based on the following data, suggest insights on movie trends and audience preferences: {user_input}\"\n",
    "\n",
    "# Prepare data sample to send to the model\n",
    "sample_input = data.head(5).to_string()\n",
    "\n",
    "# Step 3: Load OpenChat 3.5 Model\n",
    "model_name = \"openchat/openchat-3.5-0106\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "# Step 4: Apply Template 1\n",
    "input1 = TEMPLATE_1.format(user_input=sample_input)\n",
    "output1 = generator(input1, max_length=500, do_sample=True)[0]['generated_text']\n",
    "print(\"\\n--- Template 1 Output ---\\n\", output1)\n",
    "\n",
    "# Step 5: Apply Template 2\n",
    "input2 = TEMPLATE_2.format(user_input=sample_input)\n",
    "output2 = generator(input2, max_length=500, do_sample=True)[0]['generated_text']\n",
    "print(\"\\n--- Template 2 Output ---\\n\", output2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
